<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Linear Models on Practical Statistics</title>
    <link>/tags/linear-models/index.xml</link>
    <description>Recent content in Linear Models on Practical Statistics</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="/tags/linear-models/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Generalized Estimating Equations (GEE)</title>
      <link>/2017/05/10/generalized-estimating-equations-gee/</link>
      <pubDate>Wed, 10 May 2017 21:13:14 -0500</pubDate>
      
      <guid>/2017/05/10/generalized-estimating-equations-gee/</guid>
      <description>&lt;!-- BLOGDOWN-BODY-BEFORE

/BLOGDOWN-BODY-BEFORE --&gt;

&lt;div id=&#34;motivation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Motivation&lt;/h1&gt;
&lt;p&gt;Suppose we observe repeated measurements (responses and/or covariates) on a group of subjects. We’re interested in modeling the expected response for an individual based on these covariates. Some examples might include&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;assigning individuals to one of several controlled diets and measuring their cholesterol over time&lt;/li&gt;
&lt;li&gt;studying the relationship of some variable with earnings over time&lt;/li&gt;
&lt;li&gt;determining the effect of having children on a woman’s probability of participation in the labor force&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The benefit of having panel data (repeated measurements) like this is that we can control for time-invariant, unobservable differences between individuals. Having multiple observations per individual allows us to base estimates on the variation &lt;em&gt;within&lt;/em&gt; individuals.&lt;/p&gt;
&lt;p&gt;The easiest way to do answer these questions would be to fit a linear model to the data, where the covariates have an additive effect on the outcome. If the variables follow something other than a linear relationship (e.g. the response of interest is a probability), a &lt;em&gt;generalized&lt;/em&gt; linear model (GLM) would be more appropriate. GLMs have the following form: &lt;span class=&#34;math display&#34;&gt;\[Y_i = \mu_i + \varepsilon_i, \qquad g(\mu_i) = X_i&amp;#39;\beta\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where for individual &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(Y_i\)&lt;/span&gt; is the response, &lt;span class=&#34;math inline&#34;&gt;\(X_i\)&lt;/span&gt; are covariates, &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; is a vector of coefficients, &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_i\)&lt;/span&gt; is a random error term, and &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt; is a &lt;strong&gt;link function&lt;/strong&gt; that maps from the set of possible responses to a linear function of the covariates.&lt;/p&gt;
&lt;p&gt;To estimate parameters and do inference with a GLM, we must assume that errors are independent and identically distributed. With panel data, this clearly isn’t the case: observations for each individual are correlated.&lt;/p&gt;
&lt;p&gt;As we saw in an earlier presentation, one possible solution is to include &lt;a href=&#34;https://rlbarter.github.io/Practical-Statistics/2017/03/03/fixed-mixed-and-random-effects/&#34;&gt;subject-specific random effects&lt;/a&gt; in the model fitting. This method is called a &lt;strong&gt;Generalized Linear Mixed Model (GLMM)&lt;/strong&gt;. GLMMs require some parametric assumptions; if you’re like me (Kellie), assuming that everything is Gaussian probably makes you uncomfortable.&lt;/p&gt;
&lt;p&gt;Generalized estimating equations (GEE) are a nonparametric way to handle this. The idea of GEE is to average over all subjects and make a good guess on the &lt;em&gt;within-subject covariance structure&lt;/em&gt;. Instead of assuming that data were generated from a certain distribution, uses moment assumptions to iteratively choose the best &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; to describe the relationship between covariates and response.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Warning:&lt;/strong&gt; Notice that I did not specify the objective of the analysis. The interpretations of the resultingestimates are different (!) for GLMM and GEE.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;subject-specific-versus-population-averaged&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Subject-specific versus Population-averaged&lt;/h1&gt;
&lt;p&gt;GEE estimates &lt;strong&gt;population average&lt;/strong&gt; effects. Consider the following two scenarios:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Scenario 1: You are a doctor. You want to know how much a statin drug will lower your patient’s odds of getting a heart attack.&lt;/li&gt;
&lt;li&gt;Scenario 2: You are a state health official. You want to know how the number of people who die of heart attacks would change if everyone in the at-risk population took the stain drug.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Source: Allison, P. (2009)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In the first scenario, we want to know the subject-specific odds. In the second, we are interested in the prediction for the entire population. GEE can give us estimates for the second, but not the first.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;nuts-and-bolts-of-gee&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Nuts and Bolts of GEE&lt;/h1&gt;
&lt;p&gt;GEE estimates &lt;strong&gt;population-averaged&lt;/strong&gt; model parameters and their standard errors. The assumptions for GEE are similar to the assumptions for GLMs:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The responses &lt;span class=&#34;math inline&#34;&gt;\(Y_1, Y_2, \dots, Y_n\)&lt;/span&gt; are correlated or clustered&lt;/li&gt;
&lt;li&gt;There is a linear relationship between the covariates and a transformation of the response, described by the link function &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Within-subject covariance has some structure (“working covariance”):&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;independence (observations over time are independent)&lt;/li&gt;
&lt;li&gt;exchangeable (all observations over time have the same correlation)&lt;/li&gt;
&lt;li&gt;AR(1) (correlation decreases as a power of how many timepoints apart two observations are)&lt;/li&gt;
&lt;li&gt;unstructured (correlation between all timepoints may be different)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We need to pick one of these working covariance structures in order to fit the GEE. As with GLMs, GEE is done using a flavor of (iteratively reweighted least squares)[&lt;a href=&#34;https://en.wikipedia.org/wiki/Iteratively_reweighted_least_squares&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Iteratively_reweighted_least_squares&lt;/a&gt;], plugging in the working covariance matrix as a weight. The weighted least squares problems we fit are the eponymous &lt;strong&gt;estimating equations&lt;/strong&gt;. If you’re familiar with maximum likelihood, you can think of this equation as the (score function)[&lt;a href=&#34;https://en.wikipedia.org/wiki/Score_(statistics)&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Score_(statistics)&lt;/a&gt;]. This function equals 0 at the optimal choice of &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;GEE is a &lt;strong&gt;semiparametric method&lt;/strong&gt;: while we impose some structure on the data generating process (linearity), we do not fully specify its distribution. Estimating &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; is purely an exercise in optimization.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-if-i-am-worried-that-the-covariance-is-misspecified&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What if I am worried that the covariance is misspecified?&lt;/h1&gt;
&lt;p&gt;We have to pick the covariance structure in order to estimate &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;, but what if it’s not right?&lt;/p&gt;
&lt;p&gt;Since the estimating equations are really based on the first moment, &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; will be estimated consistently, even if the working covariance structure is wrong. However, the standard errors computed from this will be wrong. To fix this, use GEE with the Huber-White “sandwich estimator” for robustness. The idea behind the sandwich variance estimator is to use the empirical residuals to approximate the underlying covariance.&lt;/p&gt;
&lt;p&gt;Why bother specifying the working covariance to begin with?&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Statistical efficiency&lt;/li&gt;
&lt;li&gt;Sandwich robustness is a large-sample property&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Should we use sandwich all the time?&lt;/p&gt;
&lt;p&gt;No, it is problematic if&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The number of independent subjects is much smaller than the number of repeated measures&lt;/li&gt;
&lt;li&gt;The design is unbalanced – the number of repeated measures differs across individuals&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;example-pigs&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example: Pigs&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; How does Vitamin E and copper level in the feeds affect the weights of pigs?&lt;/p&gt;
&lt;p&gt;Data&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;weight of slaughter pigs measured weekly for 12 weeks&lt;/li&gt;
&lt;li&gt;start weight (i.e. the weight at week 1)&lt;/li&gt;
&lt;li&gt;cumulated feed intake&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Treatments (3x3 factorial design)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Vitamin E (dose: 0, 100, 200 mg dl-alpha-tocopheryl acetat/kg feed)&lt;/li&gt;
&lt;li&gt;Copper (dose: 0, 35, 175 mg/kg feed)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Source: Lauridsen, C., Højsgaard, S.,Sørensen, M.T. C. (1999).&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Implementation in R: &lt;code&gt;geepack&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;geepack&amp;quot;)
data(dietox)
dietox$Cu &amp;lt;- as.factor(dietox$Cu)
dietox$Evit &amp;lt;- as.factor(dietox$Evit)
mf &amp;lt;- formula(Weight ~ Time + Evit + Cu)
head(dietox)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Weight      Feed Time  Pig Evit Cu Litter
## 1 26.50000        NA    1 4601    1  1      1
## 2 27.59999  5.200005    2 4601    1  1      1
## 3 36.50000 17.600000    3 4601    1  1      1
## 4 40.29999 28.500000    4 4601    1  1      1
## 5 49.09998 45.200001    5 4601    1  1      1
## 6 55.39999 56.900002    6 4601    1  1      1&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;independence-working-covariance&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Independence Working Covariance&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;geeInd &amp;lt;- geeglm(mf, id=Pig, data=dietox, family=gaussian, corstr=&amp;quot;ind&amp;quot;)
summary(geeInd)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## geeglm(formula = mf, family = gaussian, data = dietox, id = Pig, 
##     corstr = &amp;quot;ind&amp;quot;)
## 
##  Coefficients:
##             Estimate  Std.err     Wald Pr(&amp;gt;|W|)    
## (Intercept) 15.07283  1.42190  112.371   &amp;lt;2e-16 ***
## Time         6.94829  0.07979 7582.549   &amp;lt;2e-16 ***
## Evit2        2.08126  1.84178    1.277    0.258    
## Evit3       -1.11327  1.84830    0.363    0.547    
## Cu2         -0.78865  1.53486    0.264    0.607    
## Cu3          1.77672  1.82134    0.952    0.329    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Estimated Scale Parameters:
##             Estimate Std.err
## (Intercept)    48.28   9.309
## 
## Correlation: Structure = independenceNumber of clusters:   72   Maximum cluster size: 12&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(geeInd)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of &amp;#39;Wald statistic&amp;#39; Table
## Model: gaussian, link: identity
## Response: Weight
## Terms added sequentially (first to last)
## 
##      Df   X2 P(&amp;gt;|Chi|)    
## Time  1 7507    &amp;lt;2e-16 ***
## Evit  2    4      0.15    
## Cu    2    2      0.41    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;exchangeable-working-covariance&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Exchangeable Working Covariance&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;geeEx &amp;lt;- geeglm(mf, id=Pig, data=dietox, family=gaussian, corstr=&amp;quot;ex&amp;quot;)
summary(geeEx)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## geeglm(formula = mf, family = gaussian, data = dietox, id = Pig, 
##     corstr = &amp;quot;ex&amp;quot;)
## 
##  Coefficients:
##             Estimate Std.err    Wald Pr(&amp;gt;|W|)    
## (Intercept)  15.0984  1.4206  112.96   &amp;lt;2e-16 ***
## Time          6.9426  0.0796 7605.79   &amp;lt;2e-16 ***
## Evit2         2.0414  1.8431    1.23     0.27    
## Evit3        -1.1103  1.8452    0.36     0.55    
## Cu2          -0.7652  1.5354    0.25     0.62    
## Cu3           1.7871  1.8189    0.97     0.33    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Estimated Scale Parameters:
##             Estimate Std.err
## (Intercept)     48.3    9.31
## 
## Correlation: Structure = exchangeable  Link = identity 
## 
## Estimated Correlation Parameters:
##       Estimate Std.err
## alpha    0.766  0.0326
## Number of clusters:   72   Maximum cluster size: 12&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(geeEx)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of &amp;#39;Wald statistic&amp;#39; Table
## Model: gaussian, link: identity
## Response: Weight
## Terms added sequentially (first to last)
## 
##      Df   X2 P(&amp;gt;|Chi|)    
## Time  1 7604    &amp;lt;2e-16 ***
## Evit  2    4      0.16    
## Cu    2    2      0.41    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;ar1-working-covariance&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;AR(1) Working Covariance&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;geeAr1 &amp;lt;- geeglm(mf, id=Pig, data=dietox, family=gaussian, corstr=&amp;quot;ar1&amp;quot;)
summary(geeAr1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## geeglm(formula = mf, family = gaussian, data = dietox, id = Pig, 
##     corstr = &amp;quot;ar1&amp;quot;)
## 
##  Coefficients:
##             Estimate Std.err    Wald Pr(&amp;gt;|W|)    
## (Intercept)  17.6124  1.3354  173.95   &amp;lt;2e-16 ***
## Time          6.7324  0.0756 7921.11   &amp;lt;2e-16 ***
## Evit2         2.3782  1.7676    1.81     0.18    
## Evit3        -0.9779  1.7369    0.32     0.57    
## Cu2          -0.3976  1.3928    0.08     0.78    
## Cu3           1.2376  1.7376    0.51     0.48    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Estimated Scale Parameters:
##             Estimate Std.err
## (Intercept)     50.5    9.41
## 
## Correlation: Structure = ar1  Link = identity 
## 
## Estimated Correlation Parameters:
##       Estimate Std.err
## alpha    0.933  0.0116
## Number of clusters:   72   Maximum cluster size: 12&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(geeAr1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of &amp;#39;Wald statistic&amp;#39; Table
## Model: gaussian, link: identity
## Response: Weight
## Terms added sequentially (first to last)
## 
##      Df   X2 P(&amp;gt;|Chi|)    
## Time  1 7907    &amp;lt;2e-16 ***
## Evit  2    5      0.07 .  
## Cu    2    1      0.65    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;advantages&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Advantages&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Computationally simple relative to MLE counterparts.&lt;/li&gt;
&lt;li&gt;No distributional assumptions.&lt;/li&gt;
&lt;li&gt;Estimates are consistent even if the correlation structure is misspecified (assuming that the model for the mean response is correct)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;limitations&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Limitations&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Likelihood-based methods are not available for usual statistical inference. GEE is a quasi-likelihood method.&lt;/li&gt;
&lt;li&gt;Unclear on how to perform model selection, as GEE is just an estimating procedure. There is no goodness-of-fit measure readily available.&lt;/li&gt;
&lt;li&gt;No subject-specific estimates; if that is the goal of your study, use a different method.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;extensions-of-gee&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Extensions of GEE&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;GEE2: second-order extension&lt;/li&gt;
&lt;li&gt;The GEE version in this presentation is GEE1.&lt;/li&gt;
&lt;li&gt;Idea: use more complex equations to study the covariance&lt;/li&gt;
&lt;li&gt;Alternating Logistic Regression (ALR) (Carey, Zeger, and Diggle (1993)): model an outcome conditional on another outcome&lt;/li&gt;
&lt;li&gt;Idea: use log odd ratios instead of correlations to model associations&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;take-home-messages-about-gee&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Take-Home Messages about GEE&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;ONLY the first the &lt;strong&gt;mean&lt;/strong&gt; and the &lt;strong&gt;covariance&lt;/strong&gt; matter (quasi-likelihood approach)&lt;/li&gt;
&lt;li&gt;Use a &lt;strong&gt;sandwich estimator&lt;/strong&gt; to guard against covariance mispecification&lt;/li&gt;
&lt;li&gt;Model &lt;strong&gt;population-averaged&lt;/strong&gt; effects&lt;/li&gt;
&lt;li&gt;Useful when the within-subject dependence is unobserved/unknown&lt;/li&gt;
&lt;li&gt;Still assume subject independence (conditioned on covariates)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;gee&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;GEE&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Liang, K., and S. L. Zeger (1986). Longitudinal data analysis using generalized linear models. Biometrika, 73:13–22.&lt;/li&gt;
&lt;li&gt;Fitzmaurice, G. M., Ware, J.H. and Laird, N. M. (2004). Applied Longitudinal Analysis. Wiley. (Chapter 13)&lt;/li&gt;
&lt;li&gt;Molenberghs, Geert and Verbeke, Geert (2005). Models for Discrete Longitudinal Data. Springer. (Chapter 8)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;to-gee-or-not-to-gee&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;To GEE or not to GEE:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Hubbard, A.E., Ahern, J., Fleischer, N.L., Van der Laan, M., Lippman, S.A., Jewell, N., Bruckner, T., Satariano, W.A. (2010). To GEE or not to GEE: comparing population average and mixed models for estimating the associations between neighborhood risk factors and health. Epidemiology 21:467–474.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;“…We conclude that the estimation-equation approach of population average models provides a more useful approximation of the truth.”&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;subject-specific-versus-population-averaged-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Subject-specific versus Population-averaged:&lt;/h4&gt;
&lt;p&gt;Allison, P. D. (2009). Fixed Effects Regression Models (Quantitative Applications in the Social Sciences). SAGE.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;blog-post&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Blog post:&lt;/h4&gt;
&lt;p&gt;Dealing with ugly data: Generalized Estimating Equations (GEE) by BOUSTERHOUT: &lt;a href=&#34;https://wildlifesnpits.wordpress.com/2014/10/24/dealing-with-ugly-data-generalized-estimating-equations-gee/&#34; class=&#34;uri&#34;&gt;https://wildlifesnpits.wordpress.com/2014/10/24/dealing-with-ugly-data-generalized-estimating-equations-gee/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;dataset&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Dataset:&lt;/h4&gt;
&lt;p&gt;Lauridsen, C., Højsgaard, S.,Sørensen, M.T. C. (1999). Influence of Dietary Rapeseed Oli, Vitamin E, and Copper on Performance and Antioxidant and Oxidative Status of Pigs. J. Anim. Sci.77:906-916&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Available in the R package geepack&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;


&lt;!-- BLOGDOWN-HEAD




/BLOGDOWN-HEAD --&gt;
</description>
    </item>
    
    <item>
      <title>Fixed, Mixed, and Random Effects</title>
      <link>/2017/03/03/fixed-mixed-and-random-effects/</link>
      <pubDate>Fri, 03 Mar 2017 21:13:14 -0500</pubDate>
      
      <guid>/2017/03/03/fixed-mixed-and-random-effects/</guid>
      <description>&lt;!-- BLOGDOWN-BODY-BEFORE

/BLOGDOWN-BODY-BEFORE --&gt;

&lt;p&gt;Last time Practical Statistics met to try to wrap our heads around fixed, mixed, and random effects. Chelsea Zhang gave a great chalk-talk (white board marker-talk just doesn’t have the same ring to it), and I will give a brief summary of the content before going through a real-life example.&lt;/p&gt;
&lt;div id=&#34;who-uses-fixed-mixed-and-random-effects&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Who Uses Fixed, Mixed, and Random Effects?&lt;/h4&gt;
&lt;p&gt;Two main groups use these terms but are referring to different models. The economists and broadly speaking, “bio folks” are the two main crowds who use these types of models. Full disclaimer: I think about these terms in the second way, so economists should feel free to comment on anything I miss, get wrong, etc.&lt;/p&gt;
&lt;p&gt;Economists are motivated by panel data (repeated measures for individuals over time) and account for unobserved individual effects on an outcome that are fixed over time in order to better analyze the effects of time-varying variables of interest.&lt;/p&gt;
&lt;p&gt;Bio folks are motivated by data that has structure that if left unaccounted for, would violate the uncorrelated, homoscedastic error assumption needed for traditional linear models. We may be interested in how much structure is imposed by certain variables, and for other variables, we may be uninterested in learning about the structure, we are just accounting for it to better estimate other quantities.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;okay-but-what-are-fixed-mixed-and-random-effects&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Okay, but what are Fixed, Mixed, and Random Effects?&lt;/h4&gt;
&lt;p&gt;First we will look at the definitions from the bio perspective.&lt;/p&gt;
&lt;p&gt;Before we look at the formulas, let’s just jump right in with a mixed effect example, which is a situation where there are both fixed and random effects, and try to develop an intuition for what might be a fixed effect versus a random effect. If not having the equation background stresses you out, feel free to scroll past this part and come back after familiarizing yourself with the math.&lt;/p&gt;
&lt;p&gt;Suppose we are interested in measuring the total energies (TE) for the ground-state configuration of atoms. To get an estimate for this we both need to take a physical measurement (M) and use a black box calculation (C) to arrive at an answer. How could we arrive at an estimate for each atom? We will extend to the case where multiple atoms are of interest jointly later on.&lt;/p&gt;
&lt;p&gt;We expect a vague form:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[TE \sim f(M,C)+ \epsilon\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the estimate of total energy is affected by the measurement and the calculation.&lt;/p&gt;
&lt;p&gt;And we expect that many different experimental scenarios could occur:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;different labs take measurements&lt;/li&gt;
&lt;li&gt;within a lab multiple measurement equipment types could be used&lt;/li&gt;
&lt;li&gt;within a lab multiple of one equipment type could be used&lt;/li&gt;
&lt;li&gt;within a lab multiple different scientists could use the same machine&lt;/li&gt;
&lt;li&gt;a few “standard” black box calculation techniques are used in this field&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For all of these scenarios we can use mixed effects to tackle the final estimation of the total energy of interest. We can notice natural groupings in the data, and we expect each group to have some effect on the answer. How do we decide which is a fixed and random effect? We should ask ourselves two key questions:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Can the groups we see be considered the full population of possible groups we could see, or are they more like a random sample of a larger set of groups?&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Full Population: Fixed&lt;/li&gt;
&lt;li&gt;Random Sample: Random&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Do we want to estimate a coefficient for this grouping, or do we just want to account for the anticipated structure that the groups may impose on our observations?&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Y: Fixed&lt;/li&gt;
&lt;li&gt;N: Random&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So let’s answer these questions for each of the possible scenarios for our measurement example.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;different labs take measurements&lt;/li&gt;
&lt;/ul&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;If you are doing a key comparison where all the major labs are participating, then a &lt;strong&gt;fixed&lt;/strong&gt; effect probably makes sense. An argument can be made for a random effect if you truly think you have a random set of labs doing the measurement.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;We probably don’t care about an individual lab’s effect, so we could go with a &lt;strong&gt;random&lt;/strong&gt; effect.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Eek, already we have a discrepancy! Let’s persevere for now, and come back to this conflicting advice.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;within a lab multiple measurement equipment types could be used&lt;/li&gt;
&lt;/ul&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Do you have one of each of the main instrument types? If so, then a &lt;strong&gt;fixed&lt;/strong&gt; effect probably makes sense. If you have a random subset of possible instrument types, then an argument can be made for a &lt;strong&gt;random&lt;/strong&gt; effect.&lt;/li&gt;
&lt;li&gt;There may be cases where you care about an instrument type’s effect (you may be interested in which instrument type is better for measuring total energy) in which case &lt;strong&gt;fixed&lt;/strong&gt; effects all the way. Otherwise, you probably wouldn’t have a vested interest in assessing an instrument type’s effect directly and you could go with a &lt;strong&gt;random&lt;/strong&gt; effect.&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;within a lab multiple of one equipment type could be used&lt;/li&gt;
&lt;/ul&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;You have a random sample of all of that particular instruments ever produced in your lab, so use a &lt;strong&gt;random&lt;/strong&gt; effect.&lt;/li&gt;
&lt;li&gt;You don’t care about an individual instrument’s effect, so use a &lt;strong&gt;random&lt;/strong&gt; effect.&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;within a lab multiple different scientists could use the same machine&lt;/li&gt;
&lt;/ul&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;You have a random sample of all scientists, so go with a &lt;strong&gt;random&lt;/strong&gt; effect.&lt;/li&gt;
&lt;li&gt;You don’t care about an individual scientist’s effect, so you can use a &lt;strong&gt;random&lt;/strong&gt; effect.&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;a few “standard” black box calculation techniques are used in this field&lt;/li&gt;
&lt;/ul&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;There could be many different ways of calculating your quantity of interest even if there are few main techniques. Additionally, perhaps there is randomness involved in the estimation process itself. You are probably better off with a &lt;strong&gt;random&lt;/strong&gt; effect.&lt;/li&gt;
&lt;li&gt;It is hard to think of a case where you would be explicitly interested in the effect of the calculation, so let’s go with a &lt;strong&gt;random&lt;/strong&gt; effect&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now we can see that often the answer to these two questions line up, and you have a good idea of whether to use a fixed or random effect. However, we saw with the first setup that this isn’t always the case. This is part of the “art” of statistical modelling. You have to think about what you are most interested in estimating and consider what the limitations of your data are. Later on we will talk about assumptions of each effect, which can help you in your decision making process.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;great-but-you-still-havent-really-told-me-what-fixed-and-random-effects-are&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Great, but you still haven’t really told me what Fixed and Random Effects are?&lt;/h4&gt;
&lt;p&gt;Fixed and random effects partition the variability in a regression type approach for observations that are correlated. For example, we expect measurements done in the same lab to be correlated with one another. By allowing for fixed and random effects that are correlated, we “soak up” the correlated variability leaving the remaining variability as the necessary heteroscedastic, uncorrelated, and normal error that we expect, essentially relying on ordinary least squares techniques to finish the estimation. So let’s see how the regression models could look:&lt;/p&gt;
&lt;p&gt;Different labs and different calculation methods&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[TE_{l,c}= \alpha_l+\beta_c+\epsilon_{l,c}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Different labs (multiple replicates) and different scientists&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[TE_{l,s}= \alpha_l+\beta_s+\epsilon_{l,s}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Different labs and different instrument types (interested in type effect)&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[TE_{l,t}= \alpha_l+\beta_t+\epsilon_{l,t}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now we are studying multiple atoms (ah, another choice, fixed or random?)&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[TE_{a,l,t}= \lambda_a+\alpha_l+\beta_t+\epsilon_{l,t}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Maybe we think certain instruments are better at measuring certain atoms&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[TE_{a,l,c}= \lambda_a+\alpha_l+\beta_{ac}+\epsilon_{a,l,c}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;What if all possible structures above occur?&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[TE_{a,l,c,s,t}= \lambda_a+\alpha_l+\beta_{ac}+\delta_{s}+\omega_t+\epsilon_{a,l,c,s,t}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note that in all of these examples we are mainly trying to break up the value that we obtain from each experiment into parts of the measurement process that contribute variability. There are no other covariates (here we don’t have extra information about the labs, scientists, etc. ). For fixed effects we are estimating a coefficient per group. For random effects we are estimating the variance of that group.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;economists-perspective&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Economists’ Perspective&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Fixed Effect&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y_{it}=X_{it}\beta +\alpha_i+\epsilon_{it}\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;: contains covariates that are not time constant&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\alpha_i\)&lt;/span&gt;: unobserved individual level effect fixed over time (fixed effect) could also incorporate a fixed effect for time &lt;span class=&#34;math inline&#34;&gt;\(\lambda_t\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\epsilon_{it}\)&lt;/span&gt;: idiosyncratic error&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y_{it}-\bar{y_i}=(X_{it}-\bar{X_i})\beta + (\alpha_i-\bar{\alpha_i})+(\epsilon_{it}-\bar{\epsilon_i})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Because we require &lt;span class=&#34;math inline&#34;&gt;\(\alpha_i\)&lt;/span&gt; to be constant over time, &lt;span class=&#34;math inline&#34;&gt;\((\alpha_i-\bar{\alpha_i})=0\)&lt;/span&gt; Now we have reduced our problem to an ordinary least squares problem.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumptions&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;correct model specification&lt;/li&gt;
&lt;li&gt;strict exogeneity: &lt;span class=&#34;math inline&#34;&gt;\(E[\epsilon_{it}| X_i, \alpha_i]=0\)&lt;/span&gt; [In words, the idiosyncratic errors are uncorrelated with the covariates and the fixed effects.]&lt;/li&gt;
&lt;li&gt;no multicollinearity* [This is why we can’t have time constant covariates in X; they would be collinear with the fixed effect which is also time invariant.]&lt;/li&gt;
&lt;li&gt;idiosyncratic error is uncorrelated and homoscedastic [This is not really true because by detrending the errors, we have introduced some dependence, to get around this we need to use the Huber White sandwich estimator (another topic of its own) to adjust the standard errors for &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;*As an aside: Rebecca Barter gave some great intuition about why multicollinearity is an assumption that often comes up. In regression settings we are interested in estimating a coefficient that is interpreted as follows “if everything else is held constant, then this coefficient represents the change in the response due to a change in this predictor”. If two covariates are multicollinear (or highly correlated), then it doesn’t make sense to imagine keeping one fixed while we are changing the other.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Random Effect&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y_{it}=X_{it}\beta +\alpha_i+\epsilon_{it}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Add an additional (strong) assumption that the random effects &lt;span class=&#34;math inline&#34;&gt;\(\alpha_i\)&lt;/span&gt; are uncorrelated with &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. Now we allow for time constant variables in &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, but this means that we no longer get the nice property that &lt;span class=&#34;math inline&#34;&gt;\(\alpha_i-\bar{\alpha_i}=0\)&lt;/span&gt;. Therefore we need to estimate our effects via optimization which can be sensitive to starting values. In general, estimating random effects is harder than estimating fixed effects. Also, random effects are often received very skeptically in the economics literature because of the strong assumptions going into the setup.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Choosing Between Fixed and Random Effects: Connection to Shrinkage/Pooling &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;*See Chapter 14 of Wooldridge for more details&lt;/p&gt;
&lt;p&gt;In the econometric framework, random effects models act as an intermediary between pooled OLS and fixed effects models. Shrinkage/pooling can be useful to leverage information across groups, especially if certain groups have smaller sample sizes. For pooled OLS we collapse the individual effect, masking any group-level variation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y_{it}=X_{it}\beta +\alpha+\epsilon_{it}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If the unobserved effects &lt;span class=&#34;math inline&#34;&gt;\(a_i\)&lt;/span&gt; have smaller variance relative to that of the error, &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_{it}\)&lt;/span&gt;, then pooled OLS may be appropriate. On the other end of the spectrum, as the number of time intervals gets large and/or the variance of the &lt;span class=&#34;math inline&#34;&gt;\(a_i\)&lt;/span&gt; is larger relative to the variance of the error, the fixed effect model is appropriate.&lt;/p&gt;
&lt;p&gt;The random effect model lies in between, so in practice, many fit the fixed effect, random effect, and pooled OLS models and compare the results to assess where on the spectrum they may be. More formally, one can fit the fixed effect and the random effect models and then apply the Hausman test. The null hypothesis is that the random effect model is appropriate. Therefore, a rejection means that the assumptions needed for the random effect model to be valid are not met. In practice, failing to reject means that the two models are fairly close, and either can be used or that there is so much variation in the fixed effect estimates that you will have low power to find practically significant differences between the groups. It should be noted that if the assumptions underpinning the random effects models are in fact reasonably met, the estimation under this model is more efficient than in the pooled OLS case.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;connection-between-two-frameworks&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Connection Between Two Frameworks&lt;/h4&gt;
&lt;p&gt;We can consider the economists’ point of view as a formalization of the intuition of partitioning that exists in the bio point of view, giving more formal identification rules that govern what is allowed to be a fixed and random effect given our data. For example, in the bio case, random effects are often assumed to come from a normal distribution with an unknown variance to be estimated and to be uncorrelated with the other covariates in the model. There are ways to get around this (we can imagine a case where the effects might “bleed” into one another if they were correlated), but out-of-the-box implementations have this assumption built in. These assumptions should be considered when choosing between what is a fixed effect and what is a random effect, making the model selection process more similar to that of the economist.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-do-i-fit-a-mixed-effect-model&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;How do I fit a mixed effect model?&lt;/h4&gt;
&lt;p&gt;I’m going to do this in R (sorry Python users) with the package lme4. This example is from the National Institute of Standards and Technology and follows the hypothetical example above. Here we are accounting for the atom effect and the analysis method effect. However, now the response is a relative error between the estimate of the total energy and the theoretical “truth”.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;require(lme4)

## Example E28 NIST Simple GUM
## data from Exhibit 42 
perNum=rep(c(4,12,20,38,56,88),3)
atom=rep(c(&amp;quot;Be&amp;quot;,&amp;quot;Mg&amp;quot;,&amp;quot;Ca&amp;quot;,&amp;quot;Sr&amp;quot;,&amp;quot;Ba&amp;quot;,&amp;quot;Ra&amp;quot;),3)
method=c(rep(1,length(atom)/3),rep(2,length(atom)/3),rep(3,length(atom)/3))
relativeError=c(0.02019569,0.05340336,0.05203850,0.04773201,0.02748393,-0.03315584,
              -0.03183248,0.00976185,0.01599646,0.01470489,-0.00244948,-0.06014852,
              0.02024823,0.05467029,0.05566630,0.06270221,0.06102522,0.06543755)

data=as.data.frame(cbind(perNum,atom,method,relativeError))
## atom and method are factors
data$perNum=as.numeric(as.character(data$perNum))
data$relativeError=as.numeric(as.character(data$relativeError))

plot(data[which(data$method==1),&amp;quot;perNum&amp;quot;],data[which(data$method==1),&amp;quot;relativeError&amp;quot;],ylim=c(-0.1,0.2),type=&amp;quot;b&amp;quot;,col=&amp;quot;red&amp;quot;,xlab=&amp;quot;atomic number&amp;quot;,ylab=&amp;quot;relative error&amp;quot;)
lines(data[which(data$method==2),&amp;quot;perNum&amp;quot;],data[which(data$method==2),&amp;quot;relativeError&amp;quot;],type=&amp;quot;b&amp;quot;,col=&amp;quot;blue&amp;quot;)
lines(data[which(data$method==3),&amp;quot;perNum&amp;quot;],data[which(data$method==3),&amp;quot;relativeError&amp;quot;],type=&amp;quot;b&amp;quot;,col=&amp;quot;forestgreen&amp;quot;)
legend(&amp;quot;topleft&amp;quot;,col=c(&amp;quot;red&amp;quot;,&amp;quot;blue&amp;quot;,&amp;quot;forestgreen&amp;quot;),c(&amp;quot;Method 1&amp;quot;,&amp;quot;Method 2&amp;quot;,&amp;quot;Method 3&amp;quot;),lty=1,lwd=2,bty=&amp;quot;n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;#####../content/post/2017-03-03-fixed-mixed-random-effects_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;NIST is interested in the following relationship:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[RE_{ij}=\alpha_i+\beta_j+\epsilon_{ij}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; indexes the atom and &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; indexes the analysis method.&lt;/p&gt;
&lt;p&gt;They treat the effect of atom as fixed and the analysis method as random. To understand NIST’s motivations a bit better, and therefore their modeling choices, the goal is to understand for each atom, what we can expect for a relative error from a subset of reasonable analysis method implementations. They assume that the analysis method effect is drawn from a normal distribution with mean zero and some unknown variance.&lt;/p&gt;
&lt;p&gt;Here we use the typical model structure where fixed effects are added in as you would covariates. To add a random effect &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; we add &lt;span class=&#34;math inline&#34;&gt;\((1|x)\)&lt;/span&gt; which says we are adding an intercept level random effect. It is possible to add a slope level effect, but we won’t go into that in this blog post.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod=lmer(relativeError~atom+(1|method),data=data)
summary(mod)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed model fit by REML [&amp;#39;lmerMod&amp;#39;]
## Formula: relativeError ~ atom + (1 | method)
##    Data: data
## 
## REML criterion at convergence: -47.2
## 
## Scaled residuals: 
##      Min       1Q   Median       3Q      Max 
## -1.32921 -0.42664  0.01711  0.26112  2.30624 
## 
## Random effects:
##  Groups   Name        Variance  Std.Dev.
##  method   (Intercept) 0.0009100 0.03017 
##  Residual             0.0004266 0.02065 
## Number of obs: 18, groups:  method, 3
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)  0.02869    0.02111   1.359
## atomBe      -0.02582    0.01686  -1.531
## atomCa       0.01255    0.01686   0.744
## atomMg       0.01059    0.01686   0.628
## atomRa      -0.03798    0.01686  -2.252
## atomSr       0.01303    0.01686   0.772
## 
## Correlation of Fixed Effects:
##        (Intr) atomBe atomCa atomMg atomRa
## atomBe -0.399                            
## atomCa -0.399  0.500                     
## atomMg -0.399  0.500  0.500              
## atomRa -0.399  0.500  0.500  0.500       
## atomSr -0.399  0.500  0.500  0.500  0.500&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So what have we estimated? The fixed effects for each atom are estimated. The unknown variance of the random effect for method is also estimated. Remember this makes sense with our decision making process. We care about the values of the effects for each atom so we estimate them. We only really care about accounting for the variability structure of the method choice, so we just estimate the variance.&lt;/p&gt;
&lt;p&gt;We can see that our fixed effect estimates are centered around zero with fairly small t values suggesting that these differences aren’t statistically significant. We can also see that for the random effects, the standard deviation for the residuals (within element variability) and for the method are fairly close. NIST concludes “for the alkaline earth metals at least, the dispersion of values attributable to differences between computational approximations is comparable to the intrinsic (in)accuracy of the individual approximation methods.”&lt;/p&gt;
&lt;p&gt;Just as an FYI, if you have a regular covariate (not a factor) of interest as well, you can just add that in as usual. This will also be considered a fixed effect.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data2=cbind(data,fake=rnorm(nrow(data),0,1))
mod2=lmer(relativeError~atom+fake+(1|method),data=data2)
summary(mod2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed model fit by REML [&amp;#39;lmerMod&amp;#39;]
## Formula: relativeError ~ atom + fake + (1 | method)
##    Data: data2
## 
## REML criterion at convergence: -40.3
## 
## Scaled residuals: 
##      Min       1Q   Median       3Q      Max 
## -1.25873 -0.34278 -0.04997  0.44990  1.89531 
## 
## Random effects:
##  Groups   Name        Variance  Std.Dev.
##  method   (Intercept) 0.0007128 0.02670 
##  Residual             0.0004239 0.02059 
## Number of obs: 18, groups:  method, 3
## 
## Fixed effects:
##              Estimate Std. Error t value
## (Intercept)  0.029468   0.019476   1.513
## atomBe      -0.019115   0.017642  -1.083
## atomCa       0.016122   0.017051   0.946
## atomMg       0.008776   0.016873   0.520
## atomRa      -0.039246   0.016841  -2.330
## atomSr       0.017097   0.017122   0.998
## fake        -0.007429   0.005932  -1.252
## 
## Correlation of Fixed Effects:
##        (Intr) atomBe atomCa atomMg atomRa atomSr
## atomBe -0.402                                   
## atomCa -0.420  0.520                            
## atomMg -0.433  0.449  0.477                     
## atomRa -0.433  0.457  0.482  0.502              
## atomSr -0.418  0.525  0.516  0.473  0.479       
## fake   -0.032 -0.303 -0.167  0.086  0.060 -0.190&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Assumptions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[RE_{ij}=\alpha_i+\beta_j+\epsilon_{ij}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here we assume that &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_{ij}\)&lt;/span&gt; are an iid sample from a Gaussian distribution with mean zero and standard deviation &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; and interpret it as the leftover error after we have partitioned out all of the correlated error induced by grouping structures. You can check this assumption by looking at plots of the residuals such as the QQ plot.&lt;/p&gt;
&lt;p&gt;Check for residuals with mean zero.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(resid(mod))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -3.252607e-18&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check for normality.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;qqnorm(resid(mod))
qqline(resid(mod))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;#####../content/post/2017-03-03-fixed-mixed-random-effects_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This doesn’t look great, but with small sample sizes, it probably isn’t unreasonable. We could try a log transformation of the relative errors if we feel compelled to make this a bit more normal.&lt;/p&gt;
&lt;p&gt;By treating &lt;span class=&#34;math inline&#34;&gt;\(\beta_j\)&lt;/span&gt; as a random effect we are assuming that they are a random sample from a Gaussian distribution with mean 0 and a standard deviation &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; that we estimate. We can plot the QQ plots per method, but it will be hard to assess normality with so few points.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## by method
par(mfrow=c(1,3))
qqnorm(resid(mod)[1:6])
qqline(resid(mod)[1:6])

qqnorm(resid(mod)[7:12])
qqline(resid(mod)[7:12])

qqnorm(resid(mod)[13:18])
qqline(resid(mod)[13:18])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;#####../content/post/2017-03-03-fixed-mixed-random-effects_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;By using a random effect at the intercept level we are assuming that whatever the effect of each method is, it’s going to be the same across atoms. If we didn’t think this was the case, we could look to slope varying models. This could be its own discussion/blog topic, so I won’t go into it here. We should note that unless we have good reason to believe that the slope varies and/or we have a lot of data with sufficient replication, we should avoid fitting the more complicated slope varying model.&lt;/p&gt;
&lt;p&gt;*Note: The Hausman test for the panel data can be found in plm::phtest.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Summary&lt;/h4&gt;
&lt;p&gt;In summary, we have seen how two schools of thought treat fixed and random effects, discussed when to use fixed effects and when to use random effects in both frameworks, discussed the assumptions behind the models, and seen how to implement a mixed effect model in R. Fixed and random effect models still remain a bit mysterious, but I hope that this discussion cleared up a few things. An important take away is that the terminology has fundamentally different interpretations depending on the field, so it is important to define how you interpret these models, especially in interdisciplinary collaborations. Feel free to offer suggestions to this blog post.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references-and-resources&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;References and Resources&lt;/h4&gt;
&lt;p&gt;Thanks to Chelsea Zhang for providing the resources that she found while preparing for her talk and for her suggestions and edits while I prepared this blog post.&lt;/p&gt;
&lt;p&gt;Econometric Point of View:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Introductory Econometrics: A Modern Approach by Jeffrey M. Wooldridge&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Biostatistics Point of View:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.stats.ox.ac.uk/~snijders/FixedRandomEffects.pdf&#34; class=&#34;uri&#34;&gt;https://www.stats.ox.ac.uk/~snijders/FixedRandomEffects.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;More Information on Multilevel Modeling Including Random Slopes Models:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data Analysis Using Regression and Multilevel/Hierarchical Models by Andrew Gelman and Jennifer Hill&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Confirmation that this topic is confusing:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://andrewgelman.com/2005/01/25/why_i_dont_use/&#34; class=&#34;uri&#34;&gt;http://andrewgelman.com/2005/01/25/why_i_dont_use/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Example Data and Setup:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Possolo, Antonio. “Simple Guide for Evaluating and Expressing the Uncertainty of NIST Measurement Results.” &lt;em&gt;NIST Technical Note&lt;/em&gt; (1900).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Implementation of Fits:&lt;/p&gt;
&lt;p&gt;-Linear Mixed-Effects Models Using R by Andrzej Galeck and Tomasz Burzykowski&lt;/p&gt;
&lt;/div&gt;


&lt;!-- BLOGDOWN-HEAD




/BLOGDOWN-HEAD --&gt;
</description>
    </item>
    
  </channel>
</rss>